import {
  START,
  END,
  MessagesAnnotation,
  StateGraph,
  MemorySaver,
} from "@langchain/langgraph";
import { v4 as uuidv4 } from "uuid";
import llm from "./generate_mode.js";
import readline from "node:readline/promises";
import { stdin as input, stdout as output } from "node:process";

import { trimMessages } from "@langchain/core/messages";
import { buildInMemoryRetriever } from "./rag/retriever.js";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
  PromptTemplate,
} from "@langchain/core/prompts";
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { createRetrievalChain } from "langchain/chains/retrieval";

// â‘  å®žä¾‹åŒ– Geminiï¼ˆé€Ÿåº¦ä¼˜å…ˆå¯ç”¨ "gemini-2.0-flash"ï¼›ç¨³å¦¥å¯ç”¨ "gemini-1.5-pro"ï¼‰

// === å®šä¹‰ä¸€ä¸ªå¯å¤ç”¨çš„ Promptï¼ˆå« system + åŽ†å²å ä½ï¼‰ ===
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant. Answer clearly and concisely."],
  // è¿™é‡ŒæŠŠçŠ¶æ€é‡Œçš„åŽ†å²æ¶ˆæ¯æ’è¿›æ¥
  new MessagesPlaceholder("messages"),
]);
// === ç»„è£…é“¾ï¼šprompt -> æ¨¡åž‹ ===
const chain = prompt.pipe(llm);

// RAG ç”¨ Promptï¼ˆæ³¨æ„å« {context}ï¼‰
const ragPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    [
      "You are a helpful assistant. Answer strictly based on the given CONTEXT.",
      "If the answer is not in the context, say you don't know.",
      "Use Chinese in your reply. At the end, list SOURCES (unique) from metadata.",
      "",
      "CONTEXT:",
      "{context}",
    ].join("\n"),
  ],
  // è®©å¯¹è¯åŽ†å²ä¹Ÿå‚ä¸Žï¼ˆé“¾ä¼šç”¨ `chat_history` è¿™ä¸ªå˜é‡åï¼‰
  new MessagesPlaceholder("chat_history"),
  ["human", "{input}"],
]);
// å¯é€‰ï¼šè‡ªå®šä¹‰â€œæ–‡æ¡£æ ¼å¼åŒ–â€ï¼ŒæŠŠæ¯æ®µçš„æ¥æºä¹Ÿå†™è¿›åŽ»ï¼Œä¾¿äºŽæ¨¡åž‹å¼•ç”¨
const documentPrompt = PromptTemplate.fromTemplate(
  "SOURCE: {source}\n{page_content}"
);

const retriever = await buildInMemoryRetriever();

const docChain = await createStuffDocumentsChain({
  llm, // å¤ç”¨ä½ çŽ°æœ‰çš„ Gemini Chat æ¨¡åž‹å®žä¾‹
  prompt: ragPrompt, // å¿…é¡»åŒ…å« {context}
  documentPrompt, // æŠŠæ¯æ®µçš„ source å¸¦ä¸Š
});

const ragChain = await createRetrievalChain({
  retriever,
  combineDocsChain: docChain,
});

// === å®šä¹‰è£å‰ªå™¨ï¼šæŽ§åˆ¶åŽ†å²é•¿åº¦ï¼Œé˜²æ­¢çª—å£çˆ†æŽ‰ ===
const trimmer = trimMessages({
  maxTokens: 1000, // è‡ªè¡ŒæŒ‰éœ€è°ƒæ•´ï¼›å…ˆç»™ä¸€ä¸ªå®‰å…¨ä¸Šé™
  strategy: "last", // ä¿ç•™æœ€è¿‘çš„å¯¹è¯
  includeSystem: true, // å§‹ç»ˆä¿ç•™æœ€å‰é¢çš„ system
  allowPartial: true, // å¿…è¦æ—¶å…è®¸æˆªæ–­ä¸€æ¡è¿‡é•¿æ¶ˆæ¯
  // ç®€æ˜“ä¼°ç®— token æ•°ï¼Œå¤Ÿç”¨å³å¯ï¼›è¦ç²¾ç¡®å¯æ¢æˆä¸“ç”¨ tokenizer
  tokenCounter: (msgs) => {
    const text = msgs
      .map((m) =>
        typeof m.content === "string" ? m.content : JSON.stringify(m.content)
      )
      .join(" ");
    // è‹±æ–‡å¤§çº¦ 4 å­—ç¬¦ â‰ˆ 1 tokenï¼›ä¸­æ–‡å¯æŠŠåˆ†æ¯æ”¹å°äº›ï¼ˆæ¯”å¦‚ 2ï¼‰
    return Math.ceil(text.length / 4);
  },
});

/**
 * æŠŠç´¯ç§¯çš„ messages ä¸¢ç»™æ¨¡åž‹
 * @param {typeof MessagesAnnotation.State} state
 * @returns
 */
const callModel = async (state) => {
  const trimmed = await trimmer.invoke(state.messages);
  const response = await chain.invoke({ messages: trimmed });
  return { messages: response }; // ä»ç„¶è¿”ç»™ LangGraph çš„æ¶ˆæ¯çŠ¶æ€
};
// ç»„è£…å·¥ä½œæµï¼ˆä¸€ä¸ªèŠ‚ç‚¹ï¼Œä»Ž START â†’ model â†’ ENDï¼‰
const workflow = new StateGraph(MessagesAnnotation)
  .addNode("model", callModel)
  .addEdge(START, "model")
  .addEdge("model", END);

// æ‰“å¼€å†…å­˜åž‹æ£€æŸ¥ç‚¹ï¼ˆæŒä¹…åŒ–æ¶ˆæ¯åŽ†å²ï¼‰
const app = workflow.compile({ checkpointer: new MemorySaver() });

// ä¸€ä¸ªä¾¿æ·æ–¹æ³•ï¼šæ‰§è¡Œä¸€æ¬¡ï¼Œå¹¶è¿”å›žæœ€åŽä¸€æ¡å›žå¤å’Œ threadId
/**
 *
 * @param {string} userText
 * @param {string} threadId
 * @returns
 */
export async function runTime(userText, threadId) {
  const config = { configurable: { thread_id: threadId ?? uuidv4() } }; // å…³é”®ï¼šthread_id
  const output = await app.invoke(
    { messages: [{ role: "user", content: userText }] },
    config
  );
  const last = output.messages[output.messages.length - 1];
  return { reply: last.content, threadId: config.configurable.thread_id };
}

async function main() {
  let threadId = uuidv4();
  console.log("å½“å‰çº¿ç¨‹:", threadId);
  console.log("ðŸ’¬ Chat started. Commands: /new å¼€æ–°ä¼šè¯, /exit é€€å‡º");

  // è§£é‡Šï¼šè¯»å–è¾“å…¥æµï¼Œåˆ›å»ºä¸€ä¸ªreadlineæŽ¥å£ï¼Œç”¨äºŽè¯»å–ç”¨æˆ·è¾“å…¥
  const rl = readline.createInterface({ input, output });
  while (true) {
    const q = await rl.question("> ");
    const text = q.trim();
    if (!text) continue;
    if (text === "/exit") break;
    if (text === "/new") {
      threadId = uuidv4();
      console.log("âœ… æ–°çº¿ç¨‹:", threadId);
      continue;
    }

    try {
      // === æ”¹æˆäº‹ä»¶æµï¼šä¿æŒ thread_id ä»¥ç»´æŒè®°å¿† ===
      const stream = await app.streamEvents(
        { messages: [{ role: "user", content: text }] },
        { version: "v2", configurable: { thread_id: threadId } }
      );

      let first = true;
      for await (const ev of stream) {
        if (ev.event === "on_chat_model_stream") {
          const chunk = ev.data?.chunk;
          // chunk.content å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å¯Œæ–‡æœ¬ç‰‡æ®µæ•°ç»„ï¼Œè¿™é‡Œåšå…¼å®¹æ‹¼æŽ¥
          const piece = Array.isArray(chunk?.content)
            ? chunk.content
                .map((c) => (typeof c === "string" ? c : c?.text ?? ""))
                .join("")
            : chunk?.content ?? "";
          if (first) {
            process.stdout.write("ðŸ¤–: ");
            first = false;
          }
          process.stdout.write(piece);
        }
      }
      process.stdout.write("\n");
    } catch (err) {
      console.error("è°ƒç”¨å¤±è´¥ï¼š", err);
    }
  }

  rl.close();
  console.log("ðŸ‘‹ Bye");
}

main().catch(console.error);
